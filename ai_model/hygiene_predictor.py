import torch
import torch.nn.functional as F
import numpy as np
from PIL import Image
from torchvision import transforms
from segmentation_models_pytorch import UnetPlusPlus
import os

# âœ… ì„¤ì •
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(BASE_DIR, "ai_model", "hygiene_model_saved_weight.pt")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… í´ëž˜ìŠ¤ ID â†’ ë¼ë²¨ëª… ë§¤í•‘
HYGIENE_CLASS_MAP = {
    1: "ì•„ë§ê° (am)",
    2: "ì„¸ë¼ë¯¹ (cecr)",
    3: "ê³¨ë“œ (gcr)",
    4: "ë©”íƒˆí¬ë¼ìš´ (mcr)",
    5: "êµì •ìž¥ì¹˜ (ortho)",
    6: "ì¹˜ì„ ë‹¨ê³„1 (tar1)",
    7: "ì¹˜ì„ ë‹¨ê³„2 (tar2)",
    8: "ì¹˜ì„ ë‹¨ê³„3 (tar3)",
    9: "ì§€ë¥´ì½”ë‹ˆì•„ (zircr)"
}

# âœ… ëª¨ë¸ ì •ì˜ ë° ë¡œë“œ
model = UnetPlusPlus(
    encoder_name="efficientnet-b7",
    encoder_weights=None,
    in_channels=3,
    classes=10
)
model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.to(DEVICE)
model.eval()

# âœ… ì „ì²˜ë¦¬
def preprocess(pil_img, size=(224, 224)):
    transform = transforms.Compose([
        transforms.Resize(size),
        transforms.ToTensor(),
    ])
    return transform(pil_img).unsqueeze(0)

# âœ… í›„ì²˜ë¦¬ (í´ëž˜ìŠ¤ â†’ RGBA ì»¬ëŸ¬ë§µ)
def postprocess(output_tensor, target_size=(224, 224)):
    pred = torch.argmax(output_tensor.squeeze(0), dim=0).cpu().numpy()

    PALETTE = {
        0: (0, 0, 0, 0),              # background
        1: (220, 20, 60, 200),        # ì•„ë§ê° (am): ðŸ”´ ì§„í•œ ë¹¨ê°•
        2: (138, 43, 226, 200),       # ì„¸ë¼ë¯¹ (cecr): ðŸŸ£ ë³´ë¼
        3: (255, 215, 0, 200),        # ê³¨ë“œ (gcr): ðŸŸ¡ ë…¸ëž‘ (ê³¨ë“œí†¤)
        4: (245, 245, 245, 200),      # ë©”íƒˆí¬ë¼ìš´ (mcr): âšª ê±°ì˜ í°ìƒ‰
        5: (30, 30, 30, 200),         # êµì •ìž¥ì¹˜ (ortho): âš« ì§„í•œ ê²€ì •
        6: (0, 255, 0, 200),          # ì¹˜ì„ ë‹¨ê³„1 (tar1): ðŸŸ¢ ì´ˆë¡
        7: (255, 140, 0, 200),        # ì¹˜ì„ ë‹¨ê³„2 (tar2): ðŸŸ  ì£¼í™©
        8: (0, 0, 255, 200),          # ì¹˜ì„ ë‹¨ê³„3 (tar3): ðŸ”µ íŒŒëž‘
        9: (139, 69, 19, 200),        # ì§€ë¥´ì½”ë‹ˆì•„ (zircr): ðŸŸ¤ ê°ˆìƒ‰
    }

    h, w = pred.shape
    color_mask = np.zeros((h, w, 4), dtype=np.uint8)

    for class_id, color in PALETTE.items():
        color_mask[pred == class_id] = color

    return Image.fromarray(color_mask, mode="RGBA").resize(target_size)

# âœ… ì˜ˆì¸¡ + í•©ì„± ë§ˆìŠ¤í¬ ì €ìž¥
def predict_mask_and_overlay_only(pil_img, overlay_save_path):
    input_tensor = preprocess(pil_img).to(DEVICE)
    with torch.no_grad():
        output = model(input_tensor)
        output = F.softmax(output, dim=1)

    mask_img = postprocess(output)
    resized_img = pil_img.resize(mask_img.size).convert("RGBA")
    overlayed = Image.alpha_composite(resized_img, mask_img)

    # âœ… RGBA â†’ RGB ë³€í™˜ í›„ PNGë¡œ ì €ìž¥
    overlayed = overlayed.convert("RGB")
    overlayed.save(overlay_save_path, format="PNG")
    return overlayed

# âœ… ì£¼ìš” í´ëž˜ìŠ¤ ID, confidence, ë¼ë²¨ëª… ë°˜í™˜
def get_main_class_and_confidence_and_label(pil_img):
    input_tensor = preprocess(pil_img).to(DEVICE)
    with torch.no_grad():
        output = model(input_tensor)
        output = F.softmax(output, dim=1)

    pred = torch.argmax(output.squeeze(0), dim=0).cpu().numpy()
    output_np = output.squeeze(0).cpu().numpy()

    class_ids, counts = np.unique(pred, return_counts=True)

    best_class = -1
    best_conf = 0.0
    for cid, cnt in zip(class_ids, counts):
        if cid == 0:
            continue
        class_conf = output_np[cid][pred == cid].mean()
        if cnt > 0 and class_conf > best_conf:
            best_class = cid
            best_conf = class_conf

    best_label = HYGIENE_CLASS_MAP.get(best_class, "Unknown")

    return int(best_class), float(best_conf), best_label
